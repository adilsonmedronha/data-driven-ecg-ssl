_wandb:
    value:
        cli_version: 0.19.9
        m: []
        python_version: 3.10.16
        t:
            "1":
                - 1
                - 5
                - 53
                - 55
            "2":
                - 1
                - 5
                - 53
                - 55
            "3":
                - 2
                - 13
                - 23
                - 55
            "4": 3.10.16
            "5": 0.19.9
            "8":
                - 5
            "10":
                - 3
            "12": 0.19.9
            "13": linux-x86_64
head_model:
    value:
        Description: MLP head model fine-tuned on FOUR domains of fragment. The S2V encoder was trained on WESAD dataset
        RUN_NAME: MLP_finetuned_on_four_domains
        Training_mode: Linearprobing
        batch_size: 64
        data_dir: /home/adilson/Git/SSL-BRACIS-25/Series2Vec/Dataset/Benchmarks
        dataset: Fragment
        dropout: 0.01
        epochs: 10000
        gpu: 0
        is_linear_probing: false
        key_metric: accuracy
        layers_config:
            - 640
            - 128
            - 6
        lr: 0.0001
        n_runs: 5
        seed: 1234
s2v_ssl_model:
    value:
        Data_shape:
            - 104845
            - 1
            - 720
        Model_Type: Series2Vec
        Norm: false
        Training_mode: Pre_Training
        batch_size: 64
        console: false
        data_dir: Dataset/Benchmarks
        dataset: Benchmarks
        dim_ff: 256
        dropout: 0.01
        emb_size: 16
        epochs: 600
        gpu: 0
        key_metric: accuracy
        layers: 4
        loss_module: NoFussCrossEntropyLoss()
        lr: 0.001
        model_path: Results/Pre_Training/S2V/2025-03-29_07-38/checkpoints/WESAD_pretrained_model_last.pth
        num_heads: 8
        num_labels: 5
        optimizer: |-
            RAdam (
            Parameter Group 0
                betas: (0.9, 0.999)
                buffer: [[None, None, None], [None, None, None], [None, None, None], [None, None, None], [None, None, None], [None, None, None], [None, None, None], [None, None, None], [None, None, None], [None, None, None]]
                eps: 1e-08
                lr: 0.001
                weight_decay: 0
            )
        print_interval: 10
        problem: WESAD
        rep_size: 320
        seed: 1234
        tensorboard_dir: Results/Pre_Training/S2V/2025-03-29_07-38/tb_summaries
        val_interval: 2
        val_ratio: 0.2
